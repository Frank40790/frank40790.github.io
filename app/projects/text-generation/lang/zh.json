{
  "title": "文本生成",
  "pretrain_title": "預訓練",
  "pre_what_header": "這是做什麼的？",
  "pre_what_text": "過去幾年，由於 OpenAI 推出 GPT 系列模型，語言模型備受矚目。語言建模是一個有趣的領域，可解鎖過往難以實現的可能性。我自己雖然一直在使用 GPT-2 和 GPT-3 時代的模型，卻從未嘗試過自行構建模型，所以我決定一邊學習支撐這項技術的底層架構，一邊動手打造一個。",
  "pre_arch_header": "架構",
  "pre_arch_text_1": "我先從整個編碼器—解碼器（encoder-decoder）Transformer 架構入手。當時對它完全不了解，看起來相當複雜。但我很快意識到，我只需要構建「僅解碼器」(decoder-only) 版本，於是開始從解碼器側的各個模組功能著手：文字編碼、嵌入 (embedding)、多頭注意力 (multihead attention) 與遮罩 (masking) 機制。",
  "pre_arch_citation": "圖片來源：(Vaswani, 2023)",
  "pre_learn_header": "學習歷程",
  "pre_learn_summary": "以下是我整理出的重點：",
  "pre_encoding_title": "文字編碼",
  "pre_encoding_text": "在閱讀相關資源後，我發現文字序列會透過 tokenizer 編碼成數字表示；位置編碼 (positional encoding) 則用來保留每個 token 在句子中的位置，以維持上下文意義。",
  "pre_embedding_title": "文字嵌入",
  "pre_embedding_text": "文字編碼與位置編碼的數值會被轉換成嵌入向量。嵌入模型本身需要訓練，而位置編碼的嵌入則可透過下面公式計算：",
  "pre_embedding_add_text": "文字與位置編碼的嵌入向量會相加，形成後續處理的聯合向量。",
  "pre_qkv_title": "QKV",
  "pre_qkv_text": "嵌入向量會被複製成查詢 (Query, Q)、鍵 (Key, K) 與值 (Value, V)，然後各自經過可訓練的線性層生成 Q'、K'、V'。",
  "pre_mha_title": "多頭注意力",
  "pre_mha_text": "在多頭注意力機制中，Q'、K'、V' 會套用下列公式，其中 dₖ 為（模型維度 ÷ 注意力頭數）：",
  "pre_masking_text": "接著對注意力矩陣進行遮罩，避免模型看到未來的 token。",
  "pre_masked_alt": "遮罩後的注意力",
  "pre_qkv_alt": "QKV 運算",
  "pre_code_header": "程式碼",
  "pre_code_encoding": "文字編碼",
  "pre_code_encoding_text": "我使用 OpenAI 的 tiktoken 進行文字編碼，與 GPT-2 相同。",
  "pre_code_data": "資料處理",
  "pre_code_data_text": "將資料拆分為訓練集與驗證集。",
  "pre_code_attention": "多頭注意力",
  "pre_code_attention_text": "使用 Query、Key、Value 實作多頭注意力。",
  "pre_code_dataset": "載入資料集",
  "pre_code_dataset_text": "使用 fineweb-edu（以及部分 openwebtext），將所有文字串接為單一語料，用於預訓練。",
  "pre_code_training": "訓練程式碼",
  "pre_train_header": "訓練",
  "pre_train_text_1": "完成所有程式碼實作後，我開始訓練模型。因為資源有限，只在本機 GPU 上跑了幾個 epoch，耗時相當久。",
  "pre_train_text_2": "以下是幾個模型的訓練結果，其中 model_0003.model 訓練時間最長：",
  "pre_train_text_3": "輸出結果有時不太合理……不過考量到算力，算是還過得去的成果。",
  "pre_tech_header": "使用技術",
  "pre_ref_header": "參考文獻",
  "instruct_title": "指令微調",
  "section_what_header": "這是做什麼的？",
  "section_what_text_1": "這是語言模型預訓練後的階段，主要目的是實現多輪對話，而不是一個簡單的「自動補全」機器人。",
  "section_training_header": "訓練",
  "section_training_text_1": "在指令微調中，我只替換了資料集，並將提示模板嵌入到訓練的 token 中，這相對簡單。但這並非毫無挑戰。首先，提示模板難以管理，尤其是換行符號。其次，我必須手動調整指令微調資料集的 padding，以便批次適合上下文視窗。",
  "section_training_text_2": "我決定使用下面的指令模板，因為三個井號在 GPT-2 tokenizer 中只佔一個 token，這樣更容易結束生成。",
  "section_bytheway_header": "順帶一提……",
  "section_bytheway_text": "在編寫指令微調的程式碼時，我想要一個更方便的方式來執行、訓練和測試模型。指令列參數變得越來越長，所以我決定使用 curses 來選擇要執行的功能，同時加入 tqdm 進度條來顯示進度。",
  "curses_left_alt": "Curses 執行類型",
  "curses_right_alt": "Curses 模型類型",
  "section_api_header": "OpenAI 相容 API",
  "section_api_text": "只是為了好玩，我寫了一個 API endpoint，處理來自 OpenAI 函式庫的請求，這樣我就可以將 WebUI 連接到我的模型上。它可以解釋某些東西，但我的模型還是會說 1+1=1。",
  "webui_alt": "在 WebUI 上運行"

}
