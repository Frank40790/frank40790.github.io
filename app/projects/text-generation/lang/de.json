{
  "title": "Textgenerierung",
  "pretrain_title": "Vortraining",
  "pre_what_header": "Was macht das?",
  "pre_what_text": "In den letzten Jahren haben Sprachmodelle aufgrund der Einführung der GPT-Modelle von OpenAI viel Aufmerksamkeit erlangt. Sprachmodellierung ist ein spannendes Feld mit ungeahnten Möglichkeiten. Obwohl ich Modelle aus der GPT-2- und GPT-3-Ära genutzt habe, wollte ich noch nie selbst eins bauen. Daher beschloss ich, beim Erlernen der zugrunde liegenden Architektur gleichzeitig selbst ein Modell zu erstellen.",
  "pre_arch_header": "Architektur",
  "pre_arch_text_1": "Ich begann mit der Betrachtung der vollständigen Encoder-Decoder-Transformer-Architektur. Anfangs wirkte alles sehr einschüchternd. Doch schnell erkannte ich, dass ich nur einen Decoder-Only-Transformer benötige. Also untersuchte ich die Funktionen der einzelnen Module auf der Decoder-Seite: Textcodierung, Embedding, Multihead Attention und Masking.",
  "pre_arch_citation": "Bildquelle: (Vaswani, 2023)",
  "pre_learn_header": "Lernreise",
  "pre_learn_summary": "Hier eine kurze Zusammenfassung meiner Erkenntnisse:",
  "pre_encoding_title": "Textcodierung",
  "pre_encoding_text": "Textsequenzen werden mit einem Tokenizer in numerische Repräsentationen umgewandelt. Positional Encoding bewahrt dabei die Token-Position im Satz, um den Kontext zu erhalten.",
  "pre_embedding_title": "Text-Embedding",
  "pre_embedding_text": "Die numerischen Werte aus Text- und Positionscodierung werden in Embedding-Vektoren umgewandelt. Das Embedding-Modell muss trainiert werden, während das Positions-Embedding über folgende Formel berechnet wird:",
  "pre_embedding_add_text": "Die Embedding-Vektoren von Text und Position werden zusammengeführt und ergeben einen gemeinsamen Vektor für die weitere Verarbeitung.",
  "pre_qkv_title": "QKV",
  "pre_qkv_text": "Der Embedding-Vektor wird in Query (Q), Key (K) und Value (V) dupliziert und jeweils durch trainierbare lineare Layer zu Q', K' und V' weiterverarbeitet.",
  "pre_mha_title": "Multihead-Attention",
  "pre_mha_text": "Q', K' und V' werden in der Multihead-Attention wie folgt kombiniert (dₖ = Modell-Dimension ÷ Anzahl der Attention-Heads):",
  "pre_masking_text": "Anschließend wird die Attention-Matrix maskiert, sodass keine zukünftigen Token berücksichtigt werden.",
  "pre_masked_alt": "Maskierte Attention",
  "pre_qkv_alt": "QKV-Operation",
  "pre_code_header": "Code",
  "pre_code_encoding": "Textcodierung",
  "pre_code_encoding_text": "Ich verwende OpenAIs tiktoken für die Textcodierung, identisch zu GPT-2.",
  "pre_code_data": "Datenverarbeitung",
  "pre_code_data_text": "Die Daten werden in Trainings- und Validierungssets aufgeteilt.",
  "pre_code_attention": "Multihead-Attention",
  "pre_code_attention_text": "Implementierung der Multihead-Attention mit Query, Key und Value.",
  "pre_code_dataset": "Datensatz laden",
  "pre_code_dataset_text": "Verwendet wird fineweb-edu (und etwas openwebtext), alle Texte werden zusammengeführt, um ein Pretraining-Modell zu erstellen.",
  "pre_code_training": "Training-Code",
  "pre_train_header": "Training",
  "pre_train_text_1": "Nach dem Implementieren des Codes begann ich mit dem Modelltraining. Mangels hoher Rechenressourcen lief das Training nur für wenige Epochen auf meiner lokalen GPU und dauerte recht lange.",
  "pre_train_text_2": "Hier die Ergebnisse einiger Modelle; model_0003.model wurde am längsten trainiert:",
  "pre_train_text_3": "Die Ausgaben sind nicht immer sinnvoll… aber angesichts der begrenzten Ressourcen ist das Ergebnis akzeptabel.",
  "pre_tech_header": "Verwendete Technologien",
  "pre_ref_header": "Literatur",
  "instruct_title": "Anweisung",
  "section_what_header": "Was macht das?",
  "section_what_text_1": "Dies ist die Phase nach dem Vortraining des Sprachmodells, hauptsächlich um Mehrfach-Dialoge zu ermöglichen, anstatt nur eine \"Auto-Vervollständigungs\"-Funktion zu haben.",
  "section_training_header": "Training",
  "section_training_text_1": "Beim Instruction Tuning habe ich nur das Dataset ausgetauscht und das Prompt-Template in die Trainingstokens eingebettet, was relativ einfach ist. Es gibt jedoch einige Herausforderungen. Erstens ist das Prompt-Template schwer zu verwalten, insbesondere mit Zeilenumbrüchen. Zweitens musste ich das Padding des Datasets manuell anpassen, damit das Batch ins Kontextfenster passt.",
  "section_training_text_2": "Ich habe mich für das folgende Template entschieden, weil drei Rautenzeichen im GPT-2-Tokenizer ein Token sind, was die Beendigung der Generierung erleichtert.",
  "section_bytheway_header": "Übrigens...",
  "section_bytheway_text": "Beim Schreiben des Codes für das Instruction Tuning wollte ich eine einfachere Möglichkeit, das Modell auszuführen, zu trainieren und zu testen. Die Kommandozeilenparameter wurden zu lang, also habe ich curses verwendet, um die Funktion auszuwählen, die ich ausführen möchte, und tqdm hinzugefügt, um den Fortschritt anzuzeigen.",
  "curses_left_alt": "Curses Ausführungstyp",
  "curses_right_alt": "Curses Modelltyp",
  "section_api_header": "OpenAI-kompatible API",
  "section_api_text": "Nur zum Spaß habe ich einen API-Endpunkt geschrieben, der Anfragen der OpenAI-Bibliothek verarbeitet, damit ich eine WebUI mit meinem Modell verbinden kann. Es kann gewisse Dinge erklären, aber mein Modell sagt trotzdem, dass 1+1=1.",
  "webui_alt": "Läuft auf WebUI"

}
