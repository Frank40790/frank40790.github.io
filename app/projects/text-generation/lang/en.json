{
  "title": "Text Generation",
  "pretrain_title": "Pretrain",
  "pre_what_header": "What does this do?",
  "pre_what_text": "Language model had gain lots of attraction in the past few years due to the launch of GPT models from OpenAI. Language modelling is a interesting field to explore, it could unlock possibilities that is hard to achieve in the past. While I personally had been using the language model from GPT-2 and GPT-3 era, I have never tried to build one myself, so I decided to create one while I learn the underlying archetecture that powers this technology.",
  "pre_arch_header": "The Architecture",
  "pre_arch_text_1": "I started by looking at the full encoder-decoder transformer archetecture. Not understanding anything about it, the architecture looks very intimidating. But soon I realised that I am going to build a decoder only transformer, so I started to search up the function of each block on the decoder side, starting from the text encoding, embedding, multihead attention and the masking mechanism.",
  "pre_arch_citation": "image: (Vaswani, 2023)",
  "pre_learn_header": "Learning Journey",
  "pre_learn_summary": "This is a brief summary of what I gathered",
  "pre_encoding_title": "Text Encoding",
  "pre_encoding_text": "Starting from text encoding, after reading some resource, I found out that the text sequence is encoded using a tokenizer where the text tokens is converted into number representation. The positional encoding is for reserving the token's position in a sentence, retaining it's contextual meaning.",
  "pre_embedding_title": "Text Embedding",
  "pre_embedding_text": "The numeric data for the text encoding and position encoding is converted into a embedding vector. The embedding model has to be trained while the position encoding embedding can be calculated using the formula below.",
  "pre_embedding_add_text": "The embedded vector of the text and positional encoding is added together into a joint vector, which allows for further processing",
  "pre_qkv_title": "QKV",
  "pre_qkv_text": " The vector from the embedding are duplicated into Query (Q), Key (K) and Value (V). The Q,K and V is passed into trainable linear layer, labeled as Q', K', V'",
  "pre_mha_title": "Multihead attention",
  "pre_mha_text": " Here, the Q', K' and V' are passed into a Multihead attention where they are passed into the formula, where the dk means (model dimension / number of attention head)",
  "pre_masking_text": "Then the matrix is masked so tokens will not be able to access future tokens",
  "pre_masked_alt": "Masked attention",
  "pre_qkv_alt": "QKV",
  "pre_code_header": "The Code",
  "pre_code_encoding": "Text Encoding",
  "pre_code_encoding_text": "The text encoding I used is the tiktoken from OpenAI, using the same encoding method as GPT-2.",
  "pre_code_data": "Data Processing",
  "pre_code_data_text": "The data is split into training and validation set",
  "pre_code_attention": "Multihead Attention",
  "pre_code_attention_text": "The multihead attention using query key and value",
  "pre_code_dataset": "Loading Dataset",
  "pre_code_dataset_text": "The dataset used in this is fineweb-edu (and some openwebtext), concatenating all the text together, because this is going to be a pretrained model",
  "pre_code_training": "Training Code",
  "pre_train_header": "Training",
  "pre_train_text_1": "After all those code writing, I started to train the model. I do not have high computational resource to train this model :( , so I only trained it for a few epoch on my computer's GPU, which took quite some time to train.",
  "pre_train_text_2": "These are the result of the models, where model_0003.model is trained for the longest",
  "pre_train_text_3": "The output doesnâ€™t always make sense... but given the computational resource, it's an okay result",
  "pre_tech_header": "What is used?",
  "pre_ref_header": "References",
  "instruct_title": "Instruct",
  "section_what_header": "What does this do?",
  "section_what_text_1": "This is the stage after pretraining the language model, mainly to make multi turn conversation possible, rather than being an \"autocompletion\" bot.",
  "section_training_header": "Training",
  "section_training_text_1": "For instruction tuning, I only swapped out the dataset and embed the prompt template into the training tokens, which is relatively simple. However, this is not without any challenges. First of all, the prompt template is hard to manage, especially for the new line. Second, I have to manually adjust the padding for the instruction tuning dataset so the batch fits into the context window.",
  "section_training_text_2": "I decided to use the instruction template below, simply because triple hashtag is 1 token in GPT-2 tokenizer, which makes terminating the generation easier.",
  "section_bytheway_header": "By the way...",
  "section_bytheway_text": "When I am writing code to do instruction tuning, I want a easier way to run, train and test the model. Command line argument is getting pretty long at this point. So I decided to use curses for selecting the function I want to run, while also adding some tqdm progress bar to visualise the progress.",
  "curses_left_alt": "Curses run type",
  "curses_right_alt": "Curses model type",
  "section_api_header": "OpenAI compatiable API",
  "section_api_text": "Just for fun, I coded up a API endpoint that process the request sent by OpenAI library so that I can connect a webui to my model. It can explain something to an extent, but my model still says 1+1=1",
  "webui_alt": "Running on WebUI"
}
