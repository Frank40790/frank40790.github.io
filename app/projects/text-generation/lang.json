{
  "en": {
    "title": "Text Generation",
    "pretrain_title": "Pretrain",
    "pre_what_header": "What does this do?",
    "pre_what_text": "Language model had gain lots of attraction in the past few years due to the launch of GPT models from OpenAI. Language modelling is a interesting field to explore, it could unlock possibilities that is hard to achieve in the past. While I personally had been using the language model from GPT-2 and GPT-3 era, I have never tried to build one myself, so I decided to create one while I learn the underlying archetecture that powers this technology.",
    "pre_arch_header": "The Architecture",
    "pre_arch_text_1": "I started by looking at the full encoder-decoder transformer archetecture. Not understanding anything about it, the architecture looks very intimidating. But soon I realised that I am going to build a decoder only transformer, so I started to search up the function of each block on the decoder side, starting from the text encoding, embedding, multihead attention and the masking mechanism.",
    "pre_arch_citation": "image: (Vaswani, 2023)",
    "pre_learn_header": "Learning Journey",
    "pre_learn_summary": "This is a brief summary of what I gathered",
    "pre_encoding_title": "Text Encoding",
    "pre_encoding_text": "Starting from text encoding, after reading some resource, I found out that the text sequence is encoded using a tokenizer where the text tokens is converted into number representation. The positional encoding is for reserving the token's position in a sentence, retaining it's contextual meaning.",
    "pre_embedding_title": "Text Embedding",
    "pre_embedding_text": "The numeric data for the text encoding and position encoding is converted into a embedding vector. The embedding model has to be trained while the position encoding embedding can be calculated using the formula below.",
    "pre_embedding_add_text": "The embedded vector of the text and positional encoding is added together into a joint vector, which allows for further processing",
    "pre_qkv_title": "QKV",
    "pre_qkv_text": " The vector from the embedding are duplicated into Query (Q), Key (K) and Value (V). The Q,K and V is passed into trainable linear layer, labeled as Q', K', V'",
    "pre_mha_title": "Multihead attention",
    "pre_mha_text": " Here, the Q', K' and V' are passed into a Multihead attention where they are passed into the formula, where the dk means (model dimension / number of attention head)",
    "pre_masking_text": "Then the matrix is masked so tokens will not be able to access future tokens",
    "pre_masked_alt": "Masked attention",
    "pre_qkv_alt": "QKV",
    "pre_code_header": "The Code",
    "pre_code_encoding": "Text Encoding",
    "pre_code_encoding_text": "The text encoding I used is the tiktoken from OpenAI, using the same encoding method as GPT-2.",
    "pre_code_data": "Data Processing",
    "pre_code_data_text": "The data is split into training and validation set",
    "pre_code_attention": "Multihead Attention",
    "pre_code_attention_text": "The multihead attention using query key and value",
    "pre_code_dataset": "Loading Dataset",
    "pre_code_dataset_text": "The dataset used in this is fineweb-edu (and some openwebtext), concatenating all the text together, because this is going to be a pretrained model",
    "pre_code_training": "Training Code",
    "pre_train_header": "Training",
    "pre_train_text_1": "After all those code writing, I started to train the model. I do not have high computational resource to train this model :( , so I only trained it for a few epoch on my computer's GPU, which took quite some time to train.",
    "pre_train_text_2": "These are the result of the models, where model_0003.model is trained for the longest",
    "pre_train_text_3": "The output doesn’t always make sense... but given the computational resource, it's an okay result",
    "pre_tech_header": "What is used?",
    "pre_ref_header": "References",
    "instruct_title": "Instruct",
    "section_what_header": "What does this do?",
    "section_what_text_1": "This is the stage after pretraining the language model, mainly to make multi turn conversation possible, rather than being an \"autocompletion\" bot.",
    "section_training_header": "Training",
    "section_training_text_1": "For instruction tuning, I only swapped out the dataset and embed the prompt template into the training tokens, which is relatively simple. However, this is not without any challenges. First of all, the prompt template is hard to manage, especially for the new line. Second, I have to manually adjust the padding for the instruction tuning dataset so the batch fits into the context window.",
    "section_training_text_2": "I decided to use the instruction template below, simply because triple hashtag is 1 token in GPT-2 tokenizer, which makes terminating the generation easier.",
    "section_bytheway_header": "By the way...",
    "section_bytheway_text": "When I am writing code to do instruction tuning, I want a easier way to run, train and test the model. Command line argument is getting pretty long at this point. So I decided to use curses for selecting the function I want to run, while also adding some tqdm progress bar to visualise the progress.",
    "curses_left_alt": "Curses run type",
    "curses_right_alt": "Curses model type",
    "section_api_header": "OpenAI compatiable API",
    "section_api_text": "Just for fun, I coded up a API endpoint that process the request sent by OpenAI library so that I can connect a webui to my model. It can explain something to an extent, but my model still says 1+1=1",
    "webui_alt": "Running on WebUI"
  },
  "zh": {
    "title": "文本生成",
    "pretrain_title": "預訓練",
    "pre_what_header": "這是做什麼的？",
    "pre_what_text": "過去幾年，由於 OpenAI 推出 GPT 系列模型，語言模型備受矚目。語言建模是一個有趣的領域，可解鎖過往難以實現的可能性。我自己雖然一直在使用 GPT-2 和 GPT-3 時代的模型，卻從未嘗試過自行構建模型，所以我決定一邊學習支撐這項技術的底層架構，一邊動手打造一個。",
    "pre_arch_header": "架構",
    "pre_arch_text_1": "我先從整個編碼器—解碼器（encoder-decoder）Transformer 架構入手。當時對它完全不了解，看起來相當複雜。但我很快意識到，我只需要構建「僅解碼器」(decoder-only) 版本，於是開始從解碼器側的各個模組功能著手：文字編碼、嵌入 (embedding)、多頭注意力 (multihead attention) 與遮罩 (masking) 機制。",
    "pre_arch_citation": "圖片來源：(Vaswani, 2023)",
    "pre_learn_header": "學習歷程",
    "pre_learn_summary": "以下是我整理出的重點：",
    "pre_encoding_title": "文字編碼",
    "pre_encoding_text": "在閱讀相關資源後，我發現文字序列會透過 tokenizer 編碼成數字表示；位置編碼 (positional encoding) 則用來保留每個 token 在句子中的位置，以維持上下文意義。",
    "pre_embedding_title": "文字嵌入",
    "pre_embedding_text": "文字編碼與位置編碼的數值會被轉換成嵌入向量。嵌入模型本身需要訓練，而位置編碼的嵌入則可透過下面公式計算：",
    "pre_embedding_add_text": "文字與位置編碼的嵌入向量會相加，形成後續處理的聯合向量。",
    "pre_qkv_title": "QKV",
    "pre_qkv_text": "嵌入向量會被複製成查詢 (Query, Q)、鍵 (Key, K) 與值 (Value, V)，然後各自經過可訓練的線性層生成 Q'、K'、V'。",
    "pre_mha_title": "多頭注意力",
    "pre_mha_text": "在多頭注意力機制中，Q'、K'、V' 會套用下列公式，其中 dₖ 為（模型維度 ÷ 注意力頭數）：",
    "pre_masking_text": "接著對注意力矩陣進行遮罩，避免模型看到未來的 token。",
    "pre_masked_alt": "遮罩後的注意力",
    "pre_qkv_alt": "QKV 運算",
    "pre_code_header": "程式碼",
    "pre_code_encoding": "文字編碼",
    "pre_code_encoding_text": "我使用 OpenAI 的 tiktoken 進行文字編碼，與 GPT-2 相同。",
    "pre_code_data": "資料處理",
    "pre_code_data_text": "將資料拆分為訓練集與驗證集。",
    "pre_code_attention": "多頭注意力",
    "pre_code_attention_text": "使用 Query、Key、Value 實作多頭注意力。",
    "pre_code_dataset": "載入資料集",
    "pre_code_dataset_text": "使用 fineweb-edu（以及部分 openwebtext），將所有文字串接為單一語料，用於預訓練。",
    "pre_code_training": "訓練程式碼",
    "pre_train_header": "訓練",
    "pre_train_text_1": "完成所有程式碼實作後，我開始訓練模型。因為資源有限，只在本機 GPU 上跑了幾個 epoch，耗時相當久。",
    "pre_train_text_2": "以下是幾個模型的訓練結果，其中 model_0003.model 訓練時間最長：",
    "pre_train_text_3": "輸出結果有時不太合理……不過考量到算力，算是還過得去的成果。",
    "pre_tech_header": "使用技術",
    "pre_ref_header": "參考文獻",
    "instruct_title": "指令微調",
    "section_what_header": "這是做什麼的？",
    "section_what_text_1": "這是語言模型預訓練後的階段，主要目的是實現多輪對話，而不是一個簡單的「自動補全」機器人。",
    "section_training_header": "訓練",
    "section_training_text_1": "在指令微調中，我只替換了資料集，並將提示模板嵌入到訓練的 token 中，這相對簡單。但這並非毫無挑戰。首先，提示模板難以管理，尤其是換行符號。其次，我必須手動調整指令微調資料集的 padding，以便批次適合上下文視窗。",
    "section_training_text_2": "我決定使用下面的指令模板，因為三個井號在 GPT-2 tokenizer 中只佔一個 token，這樣更容易結束生成。",
    "section_bytheway_header": "順帶一提……",
    "section_bytheway_text": "在編寫指令微調的程式碼時，我想要一個更方便的方式來執行、訓練和測試模型。指令列參數變得越來越長，所以我決定使用 curses 來選擇要執行的功能，同時加入 tqdm 進度條來顯示進度。",
    "curses_left_alt": "Curses 執行類型",
    "curses_right_alt": "Curses 模型類型",
    "section_api_header": "OpenAI 相容 API",
    "section_api_text": "只是為了好玩，我寫了一個 API endpoint，處理來自 OpenAI 函式庫的請求，這樣我就可以將 WebUI 連接到我的模型上。它可以解釋某些東西，但我的模型還是會說 1+1=1。",
    "webui_alt": "在 WebUI 上運行"
  },
  "de": {
    "title": "Textgenerierung",
    "pretrain_title": "Vortraining",
    "pre_what_header": "Was macht das?",
    "pre_what_text": "In den letzten Jahren haben Sprachmodelle aufgrund der Einführung der GPT-Modelle von OpenAI viel Aufmerksamkeit erlangt. Sprachmodellierung ist ein spannendes Feld mit ungeahnten Möglichkeiten. Obwohl ich Modelle aus der GPT-2- und GPT-3-Ära genutzt habe, wollte ich noch nie selbst eins bauen. Daher beschloss ich, beim Erlernen der zugrunde liegenden Architektur gleichzeitig selbst ein Modell zu erstellen.",
    "pre_arch_header": "Architektur",
    "pre_arch_text_1": "Ich begann mit der Betrachtung der vollständigen Encoder-Decoder-Transformer-Architektur. Anfangs wirkte alles sehr einschüchternd. Doch schnell erkannte ich, dass ich nur einen Decoder-Only-Transformer benötige. Also untersuchte ich die Funktionen der einzelnen Module auf der Decoder-Seite: Textcodierung, Embedding, Multihead Attention und Masking.",
    "pre_arch_citation": "Bildquelle: (Vaswani, 2023)",
    "pre_learn_header": "Lernreise",
    "pre_learn_summary": "Hier eine kurze Zusammenfassung meiner Erkenntnisse:",
    "pre_encoding_title": "Textcodierung",
    "pre_encoding_text": "Textsequenzen werden mit einem Tokenizer in numerische Repräsentationen umgewandelt. Positional Encoding bewahrt dabei die Token-Position im Satz, um den Kontext zu erhalten.",
    "pre_embedding_title": "Text-Embedding",
    "pre_embedding_text": "Die numerischen Werte aus Text- und Positionscodierung werden in Embedding-Vektoren umgewandelt. Das Embedding-Modell muss trainiert werden, während das Positions-Embedding über folgende Formel berechnet wird:",
    "pre_embedding_add_text": "Die Embedding-Vektoren von Text und Position werden zusammengeführt und ergeben einen gemeinsamen Vektor für die weitere Verarbeitung.",
    "pre_qkv_title": "QKV",
    "pre_qkv_text": "Der Embedding-Vektor wird in Query (Q), Key (K) und Value (V) dupliziert und jeweils durch trainierbare lineare Layer zu Q', K' und V' weiterverarbeitet.",
    "pre_mha_title": "Multihead-Attention",
    "pre_mha_text": "Q', K' und V' werden in der Multihead-Attention wie folgt kombiniert (dₖ = Modell-Dimension ÷ Anzahl der Attention-Heads):",
    "pre_masking_text": "Anschließend wird die Attention-Matrix maskiert, sodass keine zukünftigen Token berücksichtigt werden.",
    "pre_masked_alt": "Maskierte Attention",
    "pre_qkv_alt": "QKV-Operation",
    "pre_code_header": "Code",
    "pre_code_encoding": "Textcodierung",
    "pre_code_encoding_text": "Ich verwende OpenAIs tiktoken für die Textcodierung, identisch zu GPT-2.",
    "pre_code_data": "Datenverarbeitung",
    "pre_code_data_text": "Die Daten werden in Trainings- und Validierungssets aufgeteilt.",
    "pre_code_attention": "Multihead-Attention",
    "pre_code_attention_text": "Implementierung der Multihead-Attention mit Query, Key und Value.",
    "pre_code_dataset": "Datensatz laden",
    "pre_code_dataset_text": "Verwendet wird fineweb-edu (und etwas openwebtext), alle Texte werden zusammengeführt, um ein Pretraining-Modell zu erstellen.",
    "pre_code_training": "Training-Code",
    "pre_train_header": "Training",
    "pre_train_text_1": "Nach dem Implementieren des Codes begann ich mit dem Modelltraining. Mangels hoher Rechenressourcen lief das Training nur für wenige Epochen auf meiner lokalen GPU und dauerte recht lange.",
    "pre_train_text_2": "Hier die Ergebnisse einiger Modelle; model_0003.model wurde am längsten trainiert:",
    "pre_train_text_3": "Die Ausgaben sind nicht immer sinnvoll… aber angesichts der begrenzten Ressourcen ist das Ergebnis akzeptabel.",
    "pre_tech_header": "Verwendete Technologien",
    "pre_ref_header": "Literatur",
    "instruct_title": "Anweisung",
    "section_what_header": "Was macht das?",
    "section_what_text_1": "Dies ist die Phase nach dem Vortraining des Sprachmodells, hauptsächlich um Mehrfach-Dialoge zu ermöglichen, anstatt nur eine \"Auto-Vervollständigungs\"-Funktion zu haben.",
    "section_training_header": "Training",
    "section_training_text_1": "Beim Instruction Tuning habe ich nur das Dataset ausgetauscht und das Prompt-Template in die Trainingstokens eingebettet, was relativ einfach ist. Es gibt jedoch einige Herausforderungen. Erstens ist das Prompt-Template schwer zu verwalten, insbesondere mit Zeilenumbrüchen. Zweitens musste ich das Padding des Datasets manuell anpassen, damit das Batch ins Kontextfenster passt.",
    "section_training_text_2": "Ich habe mich für das folgende Template entschieden, weil drei Rautenzeichen im GPT-2-Tokenizer ein Token sind, was die Beendigung der Generierung erleichtert.",
    "section_bytheway_header": "Übrigens...",
    "section_bytheway_text": "Beim Schreiben des Codes für das Instruction Tuning wollte ich eine einfachere Möglichkeit, das Modell auszuführen, zu trainieren und zu testen. Die Kommandozeilenparameter wurden zu lang, also habe ich curses verwendet, um die Funktion auszuwählen, die ich ausführen möchte, und tqdm hinzugefügt, um den Fortschritt anzuzeigen.",
    "curses_left_alt": "Curses Ausführungstyp",
    "curses_right_alt": "Curses Modelltyp",
    "section_api_header": "OpenAI-kompatible API",
    "section_api_text": "Nur zum Spaß habe ich einen API-Endpunkt geschrieben, der Anfragen der OpenAI-Bibliothek verarbeitet, damit ich eine WebUI mit meinem Modell verbinden kann. Es kann gewisse Dinge erklären, aber mein Modell sagt trotzdem, dass 1+1=1.",
    "webui_alt": "Läuft auf WebUI"
  },
  "ja": {
    "title": "テキスト生成",
    "pretrain_title": "事前学習",
    "pre_what_header": "これは何をするもの？",
    "pre_what_text": "近年、OpenAIのGPTモデルの登場により、言語モデルが大きな注目を集めています。言語モデリングは興味深い分野で、これまで達成が難しかった可能性を切り拓きます。私はGPT-2やGPT-3のモデルを使ってきましたが、自分でモデルを作ったことはありませんでした。そこで、この技術の基盤となるアーキテクチャを学びながら、自分でモデルを作ることにしました。",
    "pre_arch_header": "アーキテクチャ",
    "pre_arch_text_1": "最初はエンコーダ・デコーダ型のトランスフォーマー全体の構造を調べました。何も分からずに見ると非常に複雑に見えましたが、私はデコーダのみのトランスフォーマーを作ると気づき、テキストのエンコーディング、埋め込み、マルチヘッドアテンション、マスキングの機能を順に調べました。",
    "pre_arch_citation": "画像：(Vaswani, 2023)",
    "pre_learn_header": "学習の歩み",
    "pre_learn_summary": "私がまとめたポイントの簡単な概要です。",
    "pre_encoding_title": "テキストエンコーディング",
    "pre_encoding_text": "テキストのシーケンスはトークナイザーで数値に変換されます。位置エンコーディングは文中のトークンの位置を保持し、文脈の意味を維持します。",
    "pre_embedding_title": "テキスト埋め込み",
    "pre_embedding_text": "テキストエンコーディングと位置エンコーディングの数値は埋め込みベクトルに変換されます。埋め込みは学習が必要ですが、位置エンコーディングは以下の式で計算されます。",
    "pre_embedding_add_text": "テキストと位置の埋め込みベクトルは加算され、次の処理へと渡されます。",
    "pre_qkv_title": "QKV",
    "pre_qkv_text": "埋め込みベクトルはクエリ（Q）、キー（K）、バリュー（V）に複製され、それぞれ学習可能な線形層を通してQ'、K'、V'に変換されます。",
    "pre_mha_title": "マルチヘッドアテンション",
    "pre_mha_text": "Q'、K'、V'はマルチヘッドアテンションに渡され、次の式に従って計算されます。ここでd_kはモデル次元数をアテンションヘッド数で割った値です。",
    "pre_masking_text": "その後、マトリックスにマスクをかけて未来のトークンにアクセスできないようにします。",
    "pre_masked_alt": "マスクされたアテンション",
    "pre_qkv_alt": "QKV",
    "pre_code_header": "コード",
    "pre_code_encoding": "テキストエンコーディング",
    "pre_code_encoding_text": "OpenAIのtiktokenを使用し、GPT-2と同じ方法でエンコードしています。",
    "pre_code_data": "データ処理",
    "pre_code_data_text": "データを訓練セットと検証セットに分割しています。",
    "pre_code_attention": "マルチヘッドアテンション",
    "pre_code_attention_text": "クエリ、キー、バリューを使ったマルチヘッドアテンションの実装。",
    "pre_code_dataset": "データセットの読み込み",
    "pre_code_dataset_text": "fineweb-eduと一部openwebtextを使用し、すべてのテキストを結合して事前学習用データとしています。",
    "pre_code_training": "訓練コード",
    "pre_train_header": "訓練",
    "pre_train_text_1": "コードを書き終えた後、モデルの訓練を開始しました。計算資源が限られているため、自分のPCのGPUで数エポックだけ訓練しましたが、時間がかかりました。",
    "pre_train_text_2": "以下は訓練したモデルの結果で、model_0003.modelが最も長く訓練しています。",
    "pre_train_text_3": "出力は常に意味のあるものではありませんが、計算資源を考慮すれば妥当な結果です。",
    "pre_tech_header": "使用技術",
    "pre_ref_header": "参考文献",
    "instruct_title": "指示微調整",
    "section_what_header": "これは何をするもの？",
    "section_what_text_1": "これは言語モデルの事前学習の後の段階で、複数ターンの対話ができるようにするものです。単なる「自動補完」ボットではありません。",
    "section_training_header": "訓練",
    "section_training_text_1": "指示微調整では、データセットを入れ替え、プロンプトテンプレートをトークンに埋め込むだけなので比較的簡単です。ただし、テンプレートの管理が難しく、特に改行が問題です。また、バッチがコンテキストウィンドウに収まるようにパディングを手動調整しました。",
    "section_training_text_2": "以下のテンプレートを使うことにしました。三連ハッシュはGPT-2トークナイザーで1トークンなので、生成終了が簡単です。",
    "section_bytheway_header": "ちなみに…",
    "section_bytheway_text": "指示微調整のコードを書くとき、実行や訓練、テストを簡単にしたかったので、コマンドライン引数が長くなってきたため、cursesを使って実行したい機能を選択できるようにし、進行状況を示すtqdmのプログレスバーも追加しました。",
    "curses_left_alt": "Curses 実行タイプ",
    "curses_right_alt": "Curses モデルタイプ",
    "section_api_header": "OpenAI互換API",
    "section_api_text": "お遊びで、OpenAIライブラリからのリクエストを処理するAPIエンドポイントを書きました。これでWebUIを自分のモデルに接続できます。ある程度説明もできますが、モデルはまだ「1+1=1」と言います。",
    "webui_alt": "WebUI上で動作"
  },
  "es": {
    "title": "Generación de texto",
    "pretrain_title": "Preentrenamiento",
    "pre_what_header": "¿Qué hace esto?",
    "pre_what_text": "En los últimos años, los modelos de lenguaje han ganado mucha atención gracias al lanzamiento de los modelos GPT de OpenAI. La modelación del lenguaje es un campo interesante que puede desbloquear posibilidades que antes eran difíciles de alcanzar. Personalmente, he usado modelos desde la era GPT-2 y GPT-3, pero nunca he intentado construir uno yo mismo, así que decidí crear uno mientras aprendo la arquitectura subyacente que impulsa esta tecnología.",
    "pre_arch_header": "La arquitectura",
    "pre_arch_text_1": "Comencé mirando la arquitectura completa de transformador codificador-decodificador. Sin entender nada, la arquitectura parecía muy intimidante. Pero pronto me di cuenta de que iba a construir un transformador solo de decodificador, así que empecé a investigar la función de cada bloque en el lado del decodificador, comenzando desde la codificación del texto, embedding, atención multi-cabeza y el mecanismo de enmascaramiento.",
    "pre_arch_citation": "imagen: (Vaswani, 2023)",
    "pre_learn_header": "Camino de aprendizaje",
    "pre_learn_summary": "Este es un breve resumen de lo que aprendí.",
    "pre_encoding_title": "Codificación de texto",
    "pre_encoding_text": "Partiendo de la codificación del texto, tras leer algunos recursos, descubrí que la secuencia de texto se codifica usando un tokenizador, donde los tokens de texto se convierten en representaciones numéricas. La codificación posicional sirve para conservar la posición del token en la oración, manteniendo su significado contextual.",
    "pre_embedding_title": "Embeddings de texto",
    "pre_embedding_text": "Los datos numéricos de la codificación de texto y la codificación posicional se convierten en vectores de embedding. El modelo de embedding debe ser entrenado, mientras que el embedding de la codificación posicional puede calcularse con la fórmula que sigue.",
    "pre_embedding_add_text": "El vector embedding del texto y el de la codificación posicional se suman para formar un vector conjunto, que permite un procesamiento posterior.",
    "pre_qkv_title": "QKV",
    "pre_qkv_text": "El vector del embedding se duplica en Query (Q), Key (K) y Value (V). Q, K y V pasan por capas lineales entrenables, denominadas Q', K', V'.",
    "pre_mha_title": "Atención multi-cabeza",
    "pre_mha_text": "Aquí, Q', K' y V' pasan a través de la atención multi-cabeza, aplicando la fórmula donde dₖ significa (dimensión del modelo / número de cabezas de atención).",
    "pre_masking_text": "Luego la matriz es enmascarada para que los tokens no puedan acceder a tokens futuros.",
    "pre_masked_alt": "Atención enmascarada",
    "pre_qkv_alt": "QKV",
    "pre_code_header": "El código",
    "pre_code_encoding": "Codificación de texto",
    "pre_code_encoding_text": "Para la codificación de texto usé tiktoken de OpenAI, usando el mismo método que GPT-2.",
    "pre_code_data": "Procesamiento de datos",
    "pre_code_data_text": "Los datos se dividen en conjuntos de entrenamiento y validación.",
    "pre_code_attention": "Atención multi-cabeza",
    "pre_code_attention_text": "La atención multi-cabeza usando query, key y value.",
    "pre_code_dataset": "Carga del conjunto de datos",
    "pre_code_dataset_text": "El conjunto de datos usado es fineweb-edu (y algo de openwebtext), concatenando todo el texto porque será un modelo preentrenado.",
    "pre_code_training": "Código de entrenamiento",
    "pre_train_header": "Entrenamiento",
    "pre_train_text_1": "Después de escribir todo el código, empecé a entrenar el modelo. No cuento con muchos recursos computacionales, así que solo lo entrené por algunas épocas en la GPU de mi computadora, lo cual tomó bastante tiempo.",
    "pre_train_text_2": "Estos son los resultados de los modelos, donde model_0003.model fue entrenado por más tiempo.",
    "pre_train_text_3": "La salida no siempre tiene sentido... pero dado el recurso computacional, es un resultado aceptable.",
    "pre_tech_header": "¿Qué se usó?",
    "pre_ref_header": "Referencias",
    "instruct_title": "Instrucción",
    "section_what_header": "¿Qué hace esto?",
    "section_what_text_1": "Esta es la etapa después del preentrenamiento del modelo de lenguaje, principalmente para permitir conversaciones de múltiples turnos, en lugar de ser un bot de \"autocompletado\".",
    "section_training_header": "Entrenamiento",
    "section_training_text_1": "Para el ajuste por instrucciones, solo cambié el conjunto de datos e incorporé la plantilla del prompt en los tokens de entrenamiento, lo cual es relativamente simple. Sin embargo, no estuvo exento de desafíos. Primero, la plantilla del prompt es difícil de manejar, especialmente para las nuevas líneas. Segundo, tuve que ajustar manualmente el padding para que el lote del conjunto de datos de ajuste por instrucciones encajara en la ventana de contexto.",
    "section_training_text_2": "Decidí usar la siguiente plantilla de instrucciones, simplemente porque tres hashtags cuentan como un token en el tokenizador de GPT-2, lo que facilita terminar la generación.",
    "section_bytheway_header": "Por cierto...",
    "section_bytheway_text": "Cuando escribía código para el ajuste por instrucciones, quería una forma más fácil de ejecutar, entrenar y probar el modelo. Los argumentos de línea de comando se estaban volviendo muy largos, así que decidí usar curses para seleccionar la función a ejecutar, además de agregar una barra de progreso tqdm para visualizar el avance.",
    "curses_left_alt": "Tipo de ejecución Curses",
    "curses_right_alt": "Tipo de modelo Curses",
    "section_api_header": "API compatible con OpenAI",
    "section_api_text": "Solo por diversión, codifiqué un endpoint API que procesa las solicitudes enviadas por la biblioteca OpenAI para poder conectar una interfaz web a mi modelo. Puede explicar algunas cosas hasta cierto punto, pero mi modelo todavía dice que 1+1=1.",
    "webui_alt": "Ejecutándose en WebUI"
  },
  "ko": {
    "title": "텍스트 생성",
    "pretrain_title": "사전 학습",
    "pre_what_header": "이게 무엇인가요?",
    "pre_what_text": "최근 오픈AI의 GPT 모델 출시로 인해 언어 모델이 큰 주목을 받고 있습니다. 언어 모델링은 매우 흥미로운 분야이며, 이전에는 어려웠던 가능성을 열어줍니다. 저는 GPT-2, GPT-3 모델들을 사용해왔지만, 직접 모델을 만드는 것은 처음이었습니다. 그래서 이 기술의 기반 아키텍처를 공부하며 직접 모델을 만들어보기로 했습니다.",
    "pre_arch_header": "아키텍처",
    "pre_arch_text_1": "처음에는 인코더-디코더 구조의 트랜스포머 전체를 살펴보았습니다. 아무것도 모르는 상태에서 보면 매우 복잡해 보였지만, 결국 디코더만 사용하는 트랜스포머임을 깨닫고, 텍스트 인코딩, 임베딩, 멀티 헤드 어텐션, 마스킹 기능을 하나씩 공부했습니다.",
    "pre_arch_citation": "이미지: (Vaswani, 2023)",
    "pre_learn_header": "학습 여정",
    "pre_learn_summary": "제가 정리한 요점들 간략 요약입니다.",
    "pre_encoding_title": "텍스트 인코딩",
    "pre_encoding_text": "텍스트 시퀀스는 토크나이저를 통해 숫자 형태로 변환됩니다. 위치 인코딩은 문장 내 토큰 위치를 유지해 문맥 의미를 보존합니다.",
    "pre_embedding_title": "텍스트 임베딩",
    "pre_embedding_text": "텍스트 인코딩과 위치 인코딩 숫자는 임베딩 벡터로 변환됩니다. 임베딩 모델은 학습이 필요하지만 위치 임베딩은 아래 수식으로 계산할 수 있습니다.",
    "pre_embedding_add_text": "텍스트 임베딩 벡터와 위치 임베딩 벡터를 더하여 다음 단계로 전달합니다.",
    "pre_qkv_title": "QKV",
    "pre_qkv_text": "임베딩 벡터를 쿼리(Q), 키(K), 밸류(V)로 복제하여 학습 가능한 선형층을 통해 Q', K', V'로 변환합니다.",
    "pre_mha_title": "멀티헤드 어텐션",
    "pre_mha_text": "Q', K', V'는 멀티헤드 어텐션으로 넘어가며, 아래 수식에 따라 계산됩니다. 여기서 dₖ는 모델 차원을 어텐션 헤드 수로 나눈 값입니다.",
    "pre_masking_text": "그 후 미래 토큰에 접근하지 못하도록 마스크를 적용합니다.",
    "pre_masked_alt": "마스킹된 어텐션",
    "pre_qkv_alt": "QKV",
    "pre_code_header": "코드",
    "pre_code_encoding": "텍스트 인코딩",
    "pre_code_encoding_text": "OpenAI의 tiktoken을 사용하여 GPT-2 방식으로 인코딩합니다.",
    "pre_code_data": "데이터 처리",
    "pre_code_data_text": "데이터를 학습용과 검증용으로 분할합니다.",
    "pre_code_attention": "멀티헤드 어텐션",
    "pre_code_attention_text": "쿼리, 키, 밸류를 사용하는 멀티헤드 어텐션 구현.",
    "pre_code_dataset": "데이터셋 로딩",
    "pre_code_dataset_text": "fineweb-edu 데이터셋과 일부 openwebtext를 사용하며, 모든 텍스트를 연결하여 사전학습용 데이터로 만듭니다.",
    "pre_code_training": "학습 코드",
    "pre_train_header": "학습",
    "pre_train_text_1": "코드를 다 작성한 뒤 모델 학습을 시작했습니다. 계산 자원이 부족해 제 PC GPU에서 몇 epoch만 학습했는데 시간이 꽤 걸렸습니다.",
    "pre_train_text_2": "아래는 학습한 모델 결과로, model_0003.model이 가장 오래 학습되었습니다.",
    "pre_train_text_3": "출력이 항상 의미 있는 것은 아니지만, 계산 자원을 생각하면 괜찮은 결과입니다.",
    "pre_tech_header": "사용 기술",
    "pre_ref_header": "참고 문헌",
    "instruct_title": "지시 미세 조정",
    "section_what_header": "이게 무엇인가요?",
    "section_what_text_1": "사전 학습된 언어 모델 이후 단계로, 다중 턴 대화를 할 수 있도록 합니다. 단순한 '자동 완성' 봇이 아닙니다.",
    "section_training_header": "학습",
    "section_training_text_1": "지시 미세 조정에서는 데이터셋을 교체하고 프롬프트 템플릿을 토큰에 삽입하는 방식이라 비교적 간단합니다. 다만, 템플릿 관리가 어려웠고 특히 줄바꿈 처리가 문제였습니다. 또한 배치가 컨텍스트 윈도우에 맞도록 패딩을 수동 조정했습니다.",
    "section_training_text_2": "아래 템플릿을 사용하기로 했는데, 세 개의 해시(###)가 GPT-2 토크나이저에서 한 토큰이므로 생성 종료를 쉽게 할 수 있습니다.",
    "section_bytheway_header": "참고로...",
    "section_bytheway_text": "지시 미세 조정 코드를 작성할 때 실행, 학습, 테스트를 쉽게 하기 위해 명령행 인수가 길어져 curses를 사용해 실행할 기능을 선택할 수 있게 하고 tqdm 진행 바도 추가했습니다.",
    "curses_left_alt": "Curses 실행 유형",
    "curses_right_alt": "Curses 모델 유형",
    "section_api_header": "OpenAI 호환 API",
    "section_api_text": "장난 삼아 OpenAI 라이브러리에서 요청을 받아 처리하는 API 엔드포인트를 작성했습니다. 이를 통해 내 모델에 WebUI를 연결할 수 있습니다. 어느 정도 설명도 가능하지만 모델은 아직 '1+1=1'이라고 합니다.",
    "webui_alt": "WebUI에서 실행 중"
  }
}